{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark – Calculate Rolling 3-Day Average of Sales\n",
    "# Problem Statement\n",
    "# You have a PySpark DataFrame containing daily sales data. Write a PySpark program to calculate the rolling 3-day average sales for each date, ordered by the date column.\n",
    "\n",
    "# Sample Input (daily_sales)\n",
    "# sale_date\tsales\n",
    "# 2025-01-01\t100\n",
    "# 2025-01-02\t200\n",
    "# 2025-01-03\t300\n",
    "# 2025-01-04\t400\n",
    "# 2025-01-05\t500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d22307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b388ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 16:49:13 WARN Utils: Your hostname, neosoft-Latitude-5420 resolves to a loopback address: 127.0.1.1; using 10.0.61.246 instead (on interface wlp0s20f3)\n",
      "25/08/29 16:49:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/29 16:49:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/29 16:49:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkPrac\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3869c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"2025-01-01\", 100),\n",
    "(\"2025-01-02\", 200),\n",
    "(\"2025-01-03\", 300),\n",
    "(\"2025-01-04\", 400),\n",
    "(\"2025-01-05\", 500)\n",
    "]\n",
    "\n",
    "columns = [\"sale_date\",\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1219c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3f91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.withColumn(\"sale_date\", to_date(\"sale_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c6d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "| sale_date|sales|\n",
      "+----------+-----+\n",
      "|2025-01-01|  100|\n",
      "|2025-01-02|  200|\n",
      "|2025-01-03|  300|\n",
      "|2025-01-04|  400|\n",
      "|2025-01-05|  500|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a960553",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"sale_date\").rowsBetween(-2,0)\n",
    "sales = sales.withColumn(\"rolling_3_days\", avg(\"sales\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42dd918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 16:59:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/29 16:59:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/29 16:59:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/29 16:59:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/29 16:59:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+\n",
      "| sale_date|sales|rolling_3_days|\n",
      "+----------+-----+--------------+\n",
      "|2025-01-01|  100|         100.0|\n",
      "|2025-01-02|  200|         150.0|\n",
      "|2025-01-03|  300|         200.0|\n",
      "|2025-01-04|  400|         300.0|\n",
      "|2025-01-05|  500|         400.0|\n",
      "+----------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ac290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL – Find Customers with Increasing Purchase Amounts\n",
    "# Problem Statement\n",
    "# You have a SQL table purchases(customer_id, purchase_date, amount). \n",
    "# Write a query to find customers whose purchase amounts strictly increased with each new purchase date.\n",
    "\n",
    "# Sample Input (purchases)\n",
    "# customer_id\tpurchase_date\tamount\n",
    "# C1\t2025-01-01\t100\n",
    "# C1\t2025-01-05\t200\n",
    "# C1\t2025-01-10\t300\n",
    "# C2\t2025-01-02\t150\n",
    "# C2\t2025-01-06\t120\n",
    "# C3\t2025-01-03\t200\n",
    "# C3\t2025-01-09\t250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cfb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"C1\", \"2025-01-01\", 100),\n",
    "(\"C1\", \"2025-01-05\", 200),\n",
    "(\"C1\", \"2025-01-10\", 300),\n",
    "(\"C2\", \"2025-01-02\", 150),\n",
    "(\"C2\", \"2025-01-06\", 120),\n",
    "(\"C3\", \"2025-01-03\", 200),\n",
    "(\"C3\", \"2025-01-09\", 250)\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"purchase_date\", \"amount\"]\n",
    "\n",
    "purchases = spark.createDataFrame(data, columns)\n",
    "purchases = purchases.withColumn(\"purchase_date\", to_date(\"purchase_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "409b9091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+\n",
      "|customer_id|purchase_date|amount|\n",
      "+-----------+-------------+------+\n",
      "|         C1|   2025-01-01|   100|\n",
      "|         C1|   2025-01-05|   200|\n",
      "|         C1|   2025-01-10|   300|\n",
      "|         C2|   2025-01-02|   150|\n",
      "|         C2|   2025-01-06|   120|\n",
      "|         C3|   2025-01-03|   200|\n",
      "|         C3|   2025-01-09|   250|\n",
      "+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "413b3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.createOrReplaceTempView('purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e6361a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+-----------+\n",
      "|customer_id|purchase_date|amount|prev_amount|\n",
      "+-----------+-------------+------+-----------+\n",
      "|         C1|   2025-01-01|   100|       NULL|\n",
      "|         C1|   2025-01-05|   200|        100|\n",
      "|         C1|   2025-01-10|   300|        200|\n",
      "|         C2|   2025-01-02|   150|       NULL|\n",
      "|         C2|   2025-01-06|   120|        150|\n",
      "|         C3|   2025-01-03|   200|       NULL|\n",
      "|         C3|   2025-01-09|   250|        200|\n",
      "+-----------+-------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = spark.sql('''select \n",
    "        customer_id,\n",
    "        purchase_date,\n",
    "        amount, \n",
    "        lag(amount) over(partition by customer_id order by purchase_date)\n",
    "    as prev_amount \n",
    "    from purchases''')\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51f54516",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "with sale_lag_amount as (\n",
    "  select \n",
    "      customer_id,\n",
    "      purchase_date,\n",
    "      amount,\n",
    "      lag(amount) over(partition by customer_id order by purchase_date) AS prev_amount\n",
    "  from purchases\n",
    ")\n",
    "select customer_id\n",
    "from sale_lag_amount\n",
    "group by customer_id\n",
    "having count(case when prev_amount is not null and amount <= prev_amount then 1 end) = 0;\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76cb4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|         C1|\n",
      "|         C3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
